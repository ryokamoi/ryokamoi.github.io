
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a first-year CS Ph.D. student at Penn State University advised by Dr. Rui Zhang. I received my master’s degree in CS from UT Austin where I was advised by Dr. Greg Durrett, and received my bachelor’s degree in Statistics from Keio University where I was advised by Dr. Kei Kobayashi.\n[Google Scholar] [Semantic Scholar]  ","date":1728259200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1721400775,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a first-year CS Ph.D. student at Penn State University advised by Dr. Rui Zhang. I received my master’s degree in CS from UT Austin where I was advised by Dr.","tags":null,"title":"Ryo Kamoi","type":"authors"},{"authors":["Ryo Kamoi","Sarkar Snigdha Sarathi Das","Renze Lou","Jihyun Janice Ahn","Yilun Zhao","Xiaoxin Lu","Nan Zhang","Yusen Zhang","Ranran Haoran Zhang","Sujeeth Reddy Vummanthala","Salika Dave","Shaobo Qin","Arman Cohan","Wenpeng Yin","Rui Zhang"],"categories":[],"content":"","date":1728259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712282640,"objectID":"d8b3cd9b36ad5b1d89ec489090f6b56c","permalink":"https://ryokamoi.github.io/publication/kamoi-2024-realmistake/","publishdate":"2024-04-05T02:04:00.554112Z","relpermalink":"/publication/kamoi-2024-realmistake/","section":"publication","summary":"","tags":["NLP"],"title":"Evaluating LLMs at Detecting Errors in LLM Responses","type":"publication"},{"authors":["Jihyun Janice Ahn","Ryo Kamoi","Lu Cheng","Rui Zhang","Wenpeng Yin"],"categories":[],"content":"","date":1719446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721400775,"objectID":"974680fb55b5de43dd867a7052da2e31","permalink":"https://ryokamoi.github.io/publication/ahn-2024-directinverse/","publishdate":"2024-07-19T14:52:54.796768Z","relpermalink":"/publication/ahn-2024-directinverse/","section":"publication","summary":"","tags":["NLP"],"title":"Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation","type":"publication"},{"authors":["Ryo Kamoi","Yusen Zhang","Nan Zhang","Jiawei Han","Rui Zhang"],"categories":[],"content":"","date":1717459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717469110,"objectID":"d42acfeb4f2d6049e0d190978246d51d","permalink":"https://ryokamoi.github.io/publication/kamoi-2024-self-correction/","publishdate":"2024-06-04T02:45:08.892203Z","relpermalink":"/publication/kamoi-2024-self-correction/","section":"publication","summary":"","tags":["NLP"],"title":"When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs","type":"publication"},{"authors":["Yilun Zhao","Yitao Long","Hongjun Liu","Linyong Nan","Lyuhao Chen","Ryo Kamoi","Yixin Liu","Xiangru Tang","Rui Zhang","Arman Cohan"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700187241,"objectID":"734adf86e2f402b504aa05b069f5352a","permalink":"https://ryokamoi.github.io/publication/zhao-2023-docmatheval/","publishdate":"2023-11-17T02:14:00.903638Z","relpermalink":"/publication/zhao-2023-docmatheval/","section":"publication","summary":"","tags":["NLP"],"title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data","type":"publication"},{"authors":["Yusen Zhang","Nan Zhang","Yixin Liu","Alexander Fabbri","Junru Liu","Ryo Kamoi","Xiaoxin Lu","Caiming Xiong","Jieyu Zhao","Dragomir Radev","Kathleen McKeown","Rui Zhang"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700088919,"objectID":"e439ce262a87a855dd25ef55b61afda6","permalink":"https://ryokamoi.github.io/publication/zhang-2023-fair/","publishdate":"2023-11-15T22:55:19.233506Z","relpermalink":"/publication/zhang-2023-fair/","section":"publication","summary":"","tags":["NLP"],"title":"Fair Abstractive Summarization of Diverse Perspectives","type":"publication"},{"authors":["Ryo Kamoi","Tanya Goyal","Juan Diego Rodriguez","Greg Durrett"],"categories":[],"content":"","date":1701820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696714379,"objectID":"966fcec2dde527cdbec6b76943556b56","permalink":"https://ryokamoi.github.io/publication/kamoi-2023-wice/","publishdate":"2023-10-07T21:32:59.280863Z","relpermalink":"/publication/kamoi-2023-wice/","section":"publication","summary":"Models for textual entailment have increasingly been applied to settings like fact-checking, presupposition verification in question answering, and validating that generation models' outputs are faithful to a source. However, such applications are quite far from the settings that existing datasets are constructed in. We propose WiCE, a new textual entailment dataset centered around verifying claims in text, built on real-world claims and evidence in Wikipedia with fine-grained annotations. We collect sentences in Wikipedia that cite one or more webpages and annotate whether the content on those pages entails those sentences. Negative examples arise naturally, from slight misinterpretation of text to minor aspects of the sentence that are not attested in the evidence. Our annotations are over sub-sentence units of the hypothesis, decomposed automatically by GPT-3, each of which is labeled with a subset of evidence sentences from the source document. We show that real claims in our dataset involve challenging verification problems, and we benchmark existing approaches on this dataset. In addition, we show that reducing the complexity of claims by decomposing them by GPT-3 can improve entailment models' performance on various domains.","tags":["nlp"],"title":"WiCE: Real-World Entailment for Claims in Wikipedia","type":"publication"},{"authors":null,"categories":null,"content":"Talk about “WiCE: Real-World Entailment for Claims in Wikipedia” in Japanese.\n   ","date":1701216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701216000,"objectID":"74bdd6d666d7807467a6b06d9fec3a8c","permalink":"https://ryokamoi.github.io/talk/nlp-colloquium-nlp%E3%82%B3%E3%83%AD%E3%82%AD%E3%82%A6%E3%83%A0-ja/","publishdate":"2023-11-29T00:00:00Z","relpermalink":"/talk/nlp-colloquium-nlp%E3%82%B3%E3%83%AD%E3%82%AD%E3%82%A6%E3%83%A0-ja/","section":"event","summary":"Talk about “WiCE: Real-World Entailment for Claims in Wikipedia” in Japanese.\n   ","tags":null,"title":"NLP Colloquium (NLPコロキウム, ja)","type":"event"},{"authors":null,"categories":null,"content":"Talk about “Shortcomings of Question Answering Based Factuality Frameworks for Error Localization ” and “WiCE: Real-World Entailment for Claims in Wikipedia” in Japanese.\n   ","date":1688342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688342400,"objectID":"3fd4cf6b4e0f153e064f1335c853f9a8","permalink":"https://ryokamoi.github.io/talk/nagoya-nlp-seminar-at-nagoya-university-%E5%90%8D%E5%8F%A4%E5%B1%8Bnlp%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC-ja/","publishdate":"2023-07-03T00:00:00Z","relpermalink":"/talk/nagoya-nlp-seminar-at-nagoya-university-%E5%90%8D%E5%8F%A4%E5%B1%8Bnlp%E3%82%BB%E3%83%9F%E3%83%8A%E3%83%BC-ja/","section":"event","summary":"Talk about “Shortcomings of Question Answering Based Factuality Frameworks for Error Localization ” and “WiCE: Real-World Entailment for Claims in Wikipedia” in Japanese.\n   ","tags":null,"title":"Nagoya NLP Seminar at Nagoya University (名古屋NLPセミナー, ja)","type":"event"},{"authors":["Ryo Kamoi","Tanya Goyal","Greg Durrett"],"categories":[],"content":"   ","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684027078,"objectID":"96c03ff4f67c9f16417f8927b07992e5","permalink":"https://ryokamoi.github.io/publication/kamoi-2023-shortcomings/","publishdate":"2023-05-14T01:17:58.850182Z","relpermalink":"/publication/kamoi-2023-shortcomings/","section":"publication","summary":"Despite recent progress in abstractive summarization, models often generate summaries with factual errors. Numerous approaches to detect these errors have been proposed, the most popular of which are question answering (QA)-based factuality metrics. These have been shown to work well at predicting summary-level factuality and have potential to localize errors within summaries, but this latter capability has not been systematically evaluated in past research. In this paper, we conduct the first such analysis and find that, contrary to our expectations, QA-based frameworks fail to correctly identify error spans in generated summaries and are outperformed by trivial exact match baselines. Our analysis reveals a major reason for such poor localization: questions generated by the QG module often inherit errors from non-factual summaries which are then propagated further into downstream modules. Moreover, even human-in-the-loop question generation cannot easily offset these problems. Our experiments conclusively show that there exist fundamental issues with localization using the QA framework which cannot be fixed solely by stronger QA and QG models.","tags":["nlp"],"title":"Shortcomings of Question Answering Based Factuality Frameworks for Error Localization","type":"publication"},{"authors":["Ryo Kamoi","Takumi Iida","Kaname Tomite"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"ff19ffbda4599a8030c8f5a4120ea85b","permalink":"https://ryokamoi.github.io/publication/kamoi-2021/","publishdate":"2023-01-23T02:19:34.156061Z","relpermalink":"/publication/kamoi-2021/","section":"publication","summary":"Detecting unknown objects such as lost cargo is essential for improving the safety of self-driving cars. This is the first work focusing on reducing the computational cost of discrepancy networks for unknown object detection on monocular camera images. We propose an efficient discrepancy networks based solely on semantic segmentation, which has 50% fewer parameters and is 140% faster inference speed compared to an existing method, while improving detection performance by a large margin. In a major departure from prior work, we remove GANs from discrepancy networks. While previous studies have used GANs as a necessary component, our model outperforms them without using it. We further improve detection performance by analyzing intermediate representations and introducing feature selection and deep supervision. Our experiments on three datasets for obstacle detection show significant improvement of more than 5% in AUROC.","tags":["Out-of-Distribution Detection"],"title":"Efficient Unknown Object Detection with Discrepancy Networks for Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"Influence function for neural networks is proposed in the ICML2017 best paper (Wei Koh \u0026amp; Liang, 2017). However, to the best of my knowledge, there is no generic PyTorch implementation with reliable test codes. Based on some existing implementations, I’m developing reliable Pytorch implementation of influence function.\n https://github.com/ryokamoi/pytorch_influence_functions https://github.com/ryokamoi/test_pytorch_influence_functions  My repositories are forks of the following great work.\n https://github.com/nimarb/pytorch_influence_functions https://github.com/dedeswim/pytorch_influence_functions https://github.com/nayopu/influence_function_with_lissa  ","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"9184d4b565463623c3a0db16a396cf5b","permalink":"https://ryokamoi.github.io/post/2020-06-18/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/post/2020-06-18/","section":"post","summary":"Influence function for neural networks is proposed in the ICML2017 best paper (Wei Koh \u0026 Liang, 2017). However, to the best of my knowledge, there is no generic PyTorch implementation with reliable test codes.","tags":null,"title":"PyTorch Implementation of Influence Function","type":"post"},{"authors":["Katsuhiro Endo","Ryo Kamoi","Kenji Yasuoka"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"c5b6e1d1b061613b104c2b39304c6f20","permalink":"https://ryokamoi.github.io/publication/endo-2020/","publishdate":"2023-01-23T02:19:33.606403Z","relpermalink":"/publication/endo-2020/","section":"publication","summary":"","tags":["others"],"title":"Alternative methods for fast and stable GAN","type":"publication"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"03d33ca9da64144e529edebf4f41cbab","permalink":"https://ryokamoi.github.io/publication/kamoi-2020/","publishdate":"2023-01-23T02:19:34.019352Z","relpermalink":"/publication/kamoi-2020/","section":"publication","summary":"","tags":["Out-of-Distribution Detection"],"title":"Out-of-Distribution Detection with Likelihoods Assigned by Deep Generative Models Using Multimodal Prior Distributions","type":"publication"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"f75436fcd2aef06c7522e29e064ca06e","permalink":"https://ryokamoi.github.io/publication/kamoi-2020-a/","publishdate":"2023-01-23T02:19:33.741756Z","relpermalink":"/publication/kamoi-2020-a/","section":"publication","summary":"The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.","tags":["Out-of-Distribution Detection"],"title":"Why is the Mahalanobis Distance Effective for Anomaly Detection?","type":"publication"},{"authors":null,"categories":null,"content":"I have presented my research about implicit curve approximation in Meeting of the Minds 2019 (Undergraduate Research Symposium at CMU).\n My poster presented at Meeting of the Minds 2019 Implementation of present implicit curve approximation methods  Our method is based on the topologically faithful implicit curve approximation method proposed by Keren (2004). One shortcomming of the method is that it requires high-degree non-linear optimization. We propose a method to transform the problem into contrained least squares, which can be solved by quadratic programming.\nShortcommings of our method are\n It requires an ordering of the data points. It requires control points. The representation power is not sufficient.  References\n Keren, D. (2004). Topologically Faithful Fitting of Simple Closed Curves. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(1), 118–123. https://doi.org/10.1109/TPAMI.2004.1261095  ","date":1557532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557532800,"objectID":"a4a15c12604372976c5bdef6a6c3a8a3","permalink":"https://ryokamoi.github.io/post/2019-05-11/","publishdate":"2019-05-11T00:00:00Z","relpermalink":"/post/2019-05-11/","section":"post","summary":"I have presented my research about implicit curve approximation in Meeting of the Minds 2019 (Undergraduate Research Symposium at CMU).\n My poster presented at Meeting of the Minds 2019 Implementation of present implicit curve approximation methods  Our method is based on the topologically faithful implicit curve approximation method proposed by Keren (2004).","tags":null,"title":"Topologically Faithful Implicit Curve Approximation by Constrained Least Squares","type":"post"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"a23dc13939a7edc34438bfcbaeda708e","permalink":"https://ryokamoi.github.io/publication/kamoi-2019/","publishdate":"2023-01-23T02:19:34.306317Z","relpermalink":"/publication/kamoi-2019/","section":"publication","summary":"Recent work has shown that deep generative models assign higher likelihood to out-of-distribution inputs than to training data. We show that a factor underlying this phenomenon is a mismatch between the nature of the prior distribution and that of the data distribution, a problem found in widely used deep generative models such as VAEs and Glow. While a typical choice for a prior distribution is a standard Gaussian distribution, properties of distributions of real data sets may not be consistent with a unimodal prior distribution. This paper focuses on the relationship between the choice of a prior distribution and the likelihoods assigned to out-of-distribution inputs. We propose the use of a mixture distribution as a prior to make likelihoods assigned by deep generative models sensitive to out-of-distribution inputs. Furthermore, we explain the theoretical advantages of adopting a mixture distribution as the prior, and we present experimental results to support our claims. Finally, we demonstrate that a mixture prior lowers the out-of-distribution likelihood with respect to two pairs of real image data sets: Fashion-MNIST vs. MNIST and CI-FAR10 vs. SVHN.","tags":["Out-of-Distribution Detection"],"title":"Likelihood Assignment for Out-of-Distribution Inputs in Deep Generative Models is Sensitive to Prior Distribution Choice","type":"publication"},{"authors":null,"categories":null,"content":" “Generating Sentences from a Continuous Space” (Bowman et al., 2015)  https://github.com/ryokamoi/original_textvae Implementation of the first work on VAE for text. This model is a simple LSTM-LSTM seq2seq model with word dropout.   “Improved Variational Autoencoders for Text Modeling using Dilated Convolutions” (Yang, Hu, Salakhutdinov, \u0026amp; Taylor, 2017)  https://github.com/ryokamoi/dcnn_textvae Implementation of an improved model of VAE for text. This model uses dilated CNN as a decoder to control the capacity of the decoder.   “A Hybrid Convolutional Variational Autoencoder for Text Generation” (Semeniuta, Severyn, \u0026amp; Barth, 2017)  https://github.com/ryokamoi/hybrid_textvae Implementation of VAE for text with hybrid structure. This model tries to solve the problem called “posterior collapse” with an auxiliary task to predict a sentence with CNN without teacher forcing.    References\n Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., \u0026amp; Bengio, S. (2015). Generating Sentences from a Continuous Space. In SIGNLL Conference on Computational Natural Language Learning (CoNLL). Yang, Z., Hu, Z., Salakhutdinov, R., \u0026amp; Taylor, B.-K. (2017). Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. In International Conference on Machine Learning (ICML). Semeniuta, S., Severyn, A., \u0026amp; Barth, E. (2017). A Hybrid Convolutional Variational Autoencoder for Text Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 627–637).  ","date":1534636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534636800,"objectID":"d2fb98fdbb6e2b246fbfd979fa905ae6","permalink":"https://ryokamoi.github.io/post/2018-08-19/","publishdate":"2018-08-19T00:00:00Z","relpermalink":"/post/2018-08-19/","section":"post","summary":"“Generating Sentences from a Continuous Space” (Bowman et al., 2015)  https://github.com/ryokamoi/original_textvae Implementation of the first work on VAE for text. This model is a simple LSTM-LSTM seq2seq model with word dropout.","tags":null,"title":"Implementations of VAEs for Text","type":"post"},{"authors":null,"categories":null,"content":"export PATH=$PATH:/Users/ryokamoi/go/bin academic import --bibtex content/publication/publications.bib --publication-dir content/publication ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"64db4054076314ea164a2a46f8e1761f","permalink":"https://ryokamoi.github.io/publication/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/readme/","section":"publication","summary":"export PATH=$PATH:/Users/ryokamoi/go/bin academic import --bibtex content/publication/publications.bib --publication-dir content/publication ","tags":null,"title":"","type":"publication"}]