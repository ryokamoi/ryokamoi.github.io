
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m interested in building reliable and explainable NLP systems. I received a master’s degree in Computer Science from the University of Texas at Austin where I was advised by Professor Greg Durrett, and received a bachelor’s degree in Statistics from Keio University where I was advised by Professor Kei Kobayashi.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m interested in building reliable and explainable NLP systems. I received a master’s degree in Computer Science from the University of Texas at Austin where I was advised by Professor Greg Durrett, and received a bachelor’s degree in Statistics from Keio University where I was advised by Professor Kei Kobayashi.","tags":null,"title":"Ryo Kamoi","type":"authors"},{"authors":["Ryo Kamoi","Tanya Goyal","Greg Durrett"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674439989,"objectID":"c71c3f7c10fbdd8257712cc07225a907","permalink":"https://ryokamoi.github.io/publication/kamoi-2023/","publishdate":"2023-01-23T02:19:33.879025Z","relpermalink":"/publication/kamoi-2023/","section":"publication","summary":"Despite recent progress in abstractive sum-marization, models often generate summaries with factual errors. Numerous approaches to detect these errors have been proposed, the most popular of which are question answering (QA)-based factuality metrics. These have been shown to work well at predicting summary-level factuality and have potential to localize errors within summaries, but this latter capability has not been systematically evaluated in past research. In this paper, we conduct the first such analysis and find that, contrary to our expectations, QA-based frameworks fail to correctly identify error spans in generated summaries and are outperformed by trivial exact match baselines. Our analysis reveals a major reason for such poor localization: questions generated by the QG module often inherit errors from non-factual summaries which are then propagated further into downstream modules. Moreover, even human-in-the-loop question generation cannot easily offset these problems. Our experiments conclusively show that there exist fundamental issues with local-ization using the QA framework which cannot be fixed solely by stronger QA and QG models .","tags":["nlp"],"title":"Shortcomings of Question Answering Based Factuality Frameworks for Error Localization","type":"publication"},{"authors":["Ryo Kamoi","Tanya Goyal","Juan Diego Rodriguez","Greg Durrett"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677823340,"objectID":"966fcec2dde527cdbec6b76943556b56","permalink":"https://ryokamoi.github.io/publication/kamoi-2023-wice/","publishdate":"2023-03-03T06:02:20.640265Z","relpermalink":"/publication/kamoi-2023-wice/","section":"publication","summary":"Models for textual entailment have increasingly been applied to settings like fact-checking, presupposition verification in question answering, and validating that generation models' outputs are faithful to a source. However, such applications are quite far from the settings that existing datasets are constructed in. We propose WiCE, a new textual entailment dataset centered around verifying claims in text, built on real-world claims and evidence in Wikipedia with fine-grained annotations. We collect sentences in Wikipedia that cite one or more webpages and annotate whether the content on those pages entails those sentences. Negative examples arise naturally, from slight misinterpretation of text to minor aspects of the sentence that are not attested in the evidence. Our annotations are over sub-sentence units of the hypothesis, decomposed automatically by GPT-3, each of which is labeled with a subset of evidence sentences from the source document. We show that real claims in our dataset involve challenging verification problems, and we benchmark existing approaches on this dataset. In addition, we show that reducing the complexity of claims by decomposing them by GPT-3 can improve entailment models' performance on various domains.","tags":["nlp"],"title":"WiCE: Real-World Entailment for Claims in Wikipedia","type":"publication"},{"authors":["Ryo Kamoi","Takumi Iida","Kaname Tomite"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"ff19ffbda4599a8030c8f5a4120ea85b","permalink":"https://ryokamoi.github.io/publication/kamoi-2021/","publishdate":"2023-01-23T02:19:34.156061Z","relpermalink":"/publication/kamoi-2021/","section":"publication","summary":"Detecting unknown objects such as lost cargo is essential for improving the safety of self-driving cars. This is the first work focusing on reducing the computational cost of discrepancy networks for unknown object detection on monocular camera images. We propose an efficient discrepancy networks based solely on semantic segmentation, which has 50% fewer parameters and is 140% faster inference speed compared to an existing method, while improving detection performance by a large margin. In a major departure from prior work, we remove GANs from discrepancy networks. While previous studies have used GANs as a necessary component, our model outperforms them without using it. We further improve detection performance by analyzing intermediate representations and introducing feature selection and deep supervision. Our experiments on three datasets for obstacle detection show significant improvement of more than 5% in AUROC.","tags":["AI Safety"],"title":"Efficient Unknown Object Detection with Discrepancy Networks for Semantic Segmentation","type":"publication"},{"authors":["Katsuhiro Endo","Ryo Kamoi","Kenji Yasuoka"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"c5b6e1d1b061613b104c2b39304c6f20","permalink":"https://ryokamoi.github.io/publication/endo-2020/","publishdate":"2023-01-23T02:19:33.606403Z","relpermalink":"/publication/endo-2020/","section":"publication","summary":"","tags":["others"],"title":"Alternative methods for fast and stable GAN","type":"publication"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"03d33ca9da64144e529edebf4f41cbab","permalink":"https://ryokamoi.github.io/publication/kamoi-2020/","publishdate":"2023-01-23T02:19:34.019352Z","relpermalink":"/publication/kamoi-2020/","section":"publication","summary":"","tags":["AI Safety"],"title":"Out-of-Distribution Detection with Likelihoods Assigned by Deep Generative Models Using Multimodal Prior Distributions","type":"publication"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"f75436fcd2aef06c7522e29e064ca06e","permalink":"https://ryokamoi.github.io/publication/kamoi-2020-a/","publishdate":"2023-01-23T02:19:33.741756Z","relpermalink":"/publication/kamoi-2020-a/","section":"publication","summary":"The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.","tags":["AI Safety"],"title":"Why is the Mahalanobis Distance Effective for Anomaly Detection?","type":"publication"},{"authors":["Ryo Kamoi","Kei Kobayashi"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669572023,"objectID":"a23dc13939a7edc34438bfcbaeda708e","permalink":"https://ryokamoi.github.io/publication/kamoi-2019/","publishdate":"2023-01-23T02:19:34.306317Z","relpermalink":"/publication/kamoi-2019/","section":"publication","summary":"Recent work has shown that deep generative models assign higher likelihood to out-of-distribution inputs than to training data. We show that a factor underlying this phenomenon is a mismatch between the nature of the prior distribution and that of the data distribution, a problem found in widely used deep generative models such as VAEs and Glow. While a typical choice for a prior distribution is a standard Gaussian distribution, properties of distributions of real data sets may not be consistent with a unimodal prior distribution. This paper focuses on the relationship between the choice of a prior distribution and the likelihoods assigned to out-of-distribution inputs. We propose the use of a mixture distribution as a prior to make likelihoods assigned by deep generative models sensitive to out-of-distribution inputs. Furthermore, we explain the theoretical advantages of adopting a mixture distribution as the prior, and we present experimental results to support our claims. Finally, we demonstrate that a mixture prior lowers the out-of-distribution likelihood with respect to two pairs of real image data sets: Fashion-MNIST vs. MNIST and CI-FAR10 vs. SVHN.","tags":["AI Safety"],"title":"Likelihood Assignment for Out-of-Distribution Inputs in Deep Generative Models is Sensitive to Prior Distribution Choice","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://ryokamoi.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Example","tags":[""],"title":"Example Project","type":"project"}]