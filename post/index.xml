<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | Ryo Kamoi</title>
    <link>https://ryokamoi.github.io/post/</link>
      <atom:link href="https://ryokamoi.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>ja-Jp</language><lastBuildDate>Sun, 13 Jul 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ryokamoi.github.io/media/icon_hu950cef8d35da75ba4291b7d869913abc_1301_512x512_fill_lanczos_center_3.png</url>
      <title>Blog</title>
      <link>https://ryokamoi.github.io/post/</link>
    </image>
    
    <item>
      <title>arXiv Submission Process (2025)</title>
      <link>https://ryokamoi.github.io/post/2025-07-13-arxiv/</link>
      <pubDate>Sun, 13 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://ryokamoi.github.io/post/2025-07-13-arxiv/</guid>
      <description>&lt;p&gt;This post explains the arXiv submission process for who write papers using Overleaf.&lt;/p&gt;
&lt;h2 id=&#34;arxiv-latex-cleaner&#34;&gt;arXiv LaTeX Cleaner&lt;/h2&gt;
&lt;p&gt;I recommend to submit the LaTeX source files to arXiv, instead of just submitting the PDF. This allows readers to download the source files and also arXiv will generate the HTML version of the paper.&lt;/p&gt;
&lt;p&gt;However, you may want to remove comments and unnecessary files from the source files before uploading them to arXiv. To clean the LaTeX source files, I recommend using the &lt;a href=&#34;https://github.com/google-research/arxiv-latex-cleaner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;google-research/arxiv-latex-cleaner&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the source files from Overleaf: Menu &amp;gt; Download &amp;gt; Source&lt;/li&gt;
&lt;li&gt;Unzip the downloaded file.&lt;/li&gt;
&lt;li&gt;Run the cleaner script: &lt;code&gt;arxiv_latex_cleaner folder_name&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;download-bbl&#34;&gt;Download bbl&lt;/h2&gt;
&lt;p&gt;arXiv requires the &lt;code&gt;.bbl&lt;/code&gt; file to be included in the source files. However, Overleaf does not include the &lt;code&gt;.bbl&lt;/code&gt; file in the source files by default. You can download the &lt;code&gt;.bbl&lt;/code&gt; file from Overleaf by following these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the log window from the file icon to the right of the Recompile button.&lt;/li&gt;
&lt;li&gt;Go down to the bottom and find the &amp;ldquo;Other logs and output files&amp;rdquo; button.&lt;/li&gt;
&lt;li&gt;Click the button and find the &lt;code&gt;.bbl&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Download the &lt;code&gt;.bbl&lt;/code&gt; file and place it in the same folder as the other source files. You may need to rename the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now you can zip the cleaned source files and upload them to arXiv!&lt;/p&gt;
&lt;h2 id=&#34;latex-abstract-cleaner&#34;&gt;LaTeX Abstract Cleaner&lt;/h2&gt;
&lt;p&gt;I made a simple script to clean the abstract for arXiv submission: &lt;a href=&#34;https://github.com/ryokamoi/latex_abstract_cleaner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ryokamoi/latex_abstract_cleaner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It removes the LaTeX commands and special characters from the abstract, so you can copy and paste it into the arXiv submission form.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TACL Submission Process (2024)</title>
      <link>https://ryokamoi.github.io/post/2024-08-24-tacl/</link>
      <pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://ryokamoi.github.io/post/2024-08-24-tacl/</guid>
      <description>&lt;p&gt;&lt;strong&gt;This is a note from my experience in 2024. Please check the latest information from the official TACL website.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The TACL submission process is a bit complex compared to recent conference submissions on OpenReview, but the editors (and editor assistants) are very helpful and responsive.&lt;/p&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We received the initial decision (B-decision) within two months after the initial submission&lt;/li&gt;
&lt;li&gt;The revision process is very responsive and quick. I received responses in about one week for each revision (B-Decision revision and final submission).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;initial-submission&#34;&gt;Initial Submission&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Refer to &lt;a href=&#34;https://transacl.org/index.php/tacl/about/submissions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://transacl.org/index.php/tacl/about/submissions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;From 2024, we can add appendices to the paper, but we can only include (1) experimental settings such as pre-processing decisions, model parameters, feature templates or (2) complementary results (images, tables).
&lt;ul&gt;
&lt;li&gt;Refer to &lt;a href=&#34;https://transacl.org/index.php/tacl/announcement/view/105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://transacl.org/index.php/tacl/announcement/view/105&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;(the LaTeX template is not updated for this new policy)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preprints&#34;&gt;Preprints&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;From 2024, there is no anonymity period.&lt;/li&gt;
&lt;li&gt;There is no embargo period in TACL &lt;a href=&#34;https://direct.mit.edu/journals/pages/authors#reprints&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://direct.mit.edu/journals/pages/authors#reprints&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;b-decision-revision&#34;&gt;B-decision Revision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most accepted papers receive B-decision (conditional accept) &lt;a href=&#34;https://www.aclweb.org/adminwiki/index.php/2022Q1_Reports:_TACL_Journal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.aclweb.org/adminwiki/index.php/2022Q1_Reports:_TACL_Journal&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You will send an email with the revised version to the editors (&lt;strong&gt;not&lt;/strong&gt; on the submission system you use for the initial submission).&lt;/li&gt;
&lt;li&gt;There is no official format for the cover letter. I just made it by myself.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;final-submission&#34;&gt;Final Submission&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Refer to &lt;a href=&#34;https://transacl.org/ojs/index.php/tacl/author/instructions/proof&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://transacl.org/ojs/index.php/tacl/author/instructions/proof&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The final version should not have any tables or figures in the first page.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conference-presentation&#34;&gt;Conference Presentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can present TACL papers in the ACL conferences (optional).&lt;/li&gt;
&lt;li&gt;Each conference has its deadline for the A-decision and final version submission. TACL does not guarantee the decisions before the conference deadlines.
&lt;ul&gt;
&lt;li&gt;Example: &lt;a href=&#34;https://transacl.org/ojs/index.php/tacl/announcement/view/109&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EMNLP 2024 - TACL acceptance/final-version deadlines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;After the final submission, editorial assistants will send you emails about the conference presentation after the conference deadlines.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Implementation of Influence Function</title>
      <link>https://ryokamoi.github.io/post/2020-06-18/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://ryokamoi.github.io/post/2020-06-18/</guid>
      <description>&lt;p&gt;Influence function for neural networks is proposed in the ICML2017 best paper (Wei Koh &amp;amp; Liang, 2017). However, to the best of my knowledge, there is no generic PyTorch implementation with reliable test codes. Based on some existing implementations, I’m developing reliable Pytorch implementation of influence function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/pytorch_influence_functions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ryokamoi/pytorch_influence_functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/test_pytorch_influence_functions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ryokamoi/test_pytorch_influence_functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My repositories are forks of the following great work.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nimarb/pytorch_influence_functions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/nimarb/pytorch_influence_functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dedeswim/pytorch_influence_functions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/dedeswim/pytorch_influence_functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nayopu/influence_function_with_lissa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/nayopu/influence_function_with_lissa&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Topologically Faithful Implicit Curve Approximation by Constrained Least Squares</title>
      <link>https://ryokamoi.github.io/post/2019-05-11/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      <guid>https://ryokamoi.github.io/post/2019-05-11/</guid>
      <description>&lt;p&gt;I have presented my research about implicit curve approximation in Meeting of the Minds 2019 (Undergraduate Research Symposium at CMU).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ryokamoi.github.io/blog/assets/pdf/implicitcurve_poster.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My poster presented at Meeting of the Minds 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/implicit_curve_approximation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Implementation of present implicit curve approximation methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our method is based on the topologically faithful implicit curve approximation method proposed by Keren (2004). One shortcomming of the method is that it requires high-degree non-linear optimization. We propose a method to transform the problem into contrained least squares, which can be solved by quadratic programming.&lt;/p&gt;
&lt;p&gt;Shortcommings of our method are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It requires an ordering of the data points.&lt;/li&gt;
&lt;li&gt;It requires control points.&lt;/li&gt;
&lt;li&gt;The representation power is not sufficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Keren, D. (2004). Topologically Faithful Fitting of Simple Closed Curves. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(1), 118–123. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2004.1261095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TPAMI.2004.1261095&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Implementations of VAEs for Text</title>
      <link>https://ryokamoi.github.io/post/2018-08-19/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://ryokamoi.github.io/post/2018-08-19/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;“Generating Sentences from a Continuous Space” (Bowman et al., 2015)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/original_textvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ryokamoi/original_textvae&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implementation of the first work on VAE for text. This model is a simple LSTM-LSTM seq2seq model with word dropout.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;“Improved Variational Autoencoders for Text Modeling using Dilated Convolutions” (Yang, Hu, Salakhutdinov, &amp;amp; Taylor, 2017)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/dcnn_textvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ryokamoi/dcnn_textvae&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implementation of an improved model of VAE for text. This model uses dilated CNN as a decoder to control the capacity of the decoder.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;“A Hybrid Convolutional Variational Autoencoder for Text Generation” (Semeniuta, Severyn, &amp;amp; Barth, 2017)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ryokamoi/hybrid_textvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ryokamoi/hybrid_textvae&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implementation of VAE for text with hybrid structure. This model tries to solve the problem called “posterior collapse” with an auxiliary task to predict a sentence with CNN without teacher forcing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp;amp; Bengio, S. (2015). Generating Sentences from a Continuous Space. In SIGNLL Conference on Computational Natural Language Learning (CoNLL).&lt;/li&gt;
&lt;li&gt;Yang, Z., Hu, Z., Salakhutdinov, R., &amp;amp; Taylor, B.-K. (2017). Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. In International Conference on Machine Learning (ICML).&lt;/li&gt;
&lt;li&gt;Semeniuta, S., Severyn, A., &amp;amp; Barth, E. (2017). A Hybrid Convolutional Variational Autoencoder for Text Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 627–637).&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
